# Design a machine learning solution

## +Design a data ingestion strategy for ML projects

Extract data from a source and make it available to the Azure service

Extract, Transform, and Load **(*ETL*)**

Before being able to design the ETL or ELT process, you’ll need to identify your ***data source*** and ***data format**.*

Identify data source (storage)

- Customer Relationship Management (**CRM**) system
- transactional database like an **SQL database**
- generated by an Internet of Things (**IoT**) device.

Identify data format (storage)

- **Tabular** or **structured** data: All data has the same fields or properties, which are defined in a schema (Excel or CSV file)

- **Semi-structured** data: Instead, each data point is represented by a collection of *key-value pairs*. (loT generate a JSON object)
- **Unstructured** data: Files that don't adhere to any rules when it comes to structure. For example, documents, images, audio, and video files



## +Choose how to serve data to ML workflows

It's best practice to separate compute from storage

When you use **Azure Machine Learning**, **Azure Databricks**, or **Azure Synapse Analytics** for *model training*, there are three common options for storing data, which are easily connected to all three services:

- **Azure Blob Storage**: Cheapest option for storing data as *unstructured* data. Ideal for storing files like images, text, and JSON. Often also used to store data as CSV files, as data scientists prefer working with CSV files.
- **Azure Data Lake Storage (Gen 2)**: A more advanced version of the Azure Blob Storage. Also stores files like CSV files and images as *unstructured* data. A data lake also implements a hierarchical namespace, which means it’s easier to give someone access to a specific file or folder. Storage capacity is virtually limitless so ideal for storing large data.
- **Azure SQL Database**: Stores data as *structured* data. Data is read as a table and schema is defined when a table in the database is created. Ideal for data that doesn’t change over time.



## +Design a data ingestion pipeline

### Azure Synapse Analytics (**Azure Synapse Pipelines**)

Best for real-time analytics, machine learning, and advanced data engineering tasks.

- copy data from one source to a data store using connectors
- use **mapping data flow** or use a language like SQL, Python, or R for <u>*data transformation*</u>
- choose between different types of compute that can handle <u>*large data transformations at scale*</u>: server-less SQL pools, dedicated SQL pools, or Spark pools.



### Azure Data bricks

Best for comprehensive data warehousing, large-scale data integration, and analytics.

Azure Data-bricks allows you to define your pipelines in a notebook or use code-first tools like SQL, Python, or R to create your pipelines, which you can schedule to run. uses Spark clusters



### Azure Machine Learning

End-to-end machine learning lifecycle management.

Create a pipeline with the Designer, or by creating a collection of scripts. Can also extract, transform, and store the data in preparation for training. Uses clusters.

Whenever you want to perform *all tasks within the same tool*, creating and scheduling an Azure Machine Learning pipeline to run with the on-demand compute cluster may best suit your needs.

```
from azure.ai.ml import MLClient
from azure.ai.ml.compute import DatabricksCompute
from azure.identity import DefaultAzureCredential

# Initialize MLClient
credential = DefaultAzureCredential()
ml_client = MLClient(subscription_id, resource_group, workspace_name, credential)

# Define the Databricks compute target
databricks_compute = DatabricksCompute(
    name="my_databricks_compute",
    resource_id="<Databricks Resource ID>",
    workspace_url="<Databricks Workspace URL>",
    access_token="<Databricks Access Token>",
    cluster_id="<Databricks Cluster ID>"
)

# Create or update the Databricks compute target
ml_client.compute.create_or_update(databricks_compute)

```



### Design a data ingestion solution

data ingestion steps:

1. Extract raw data from its source (like a CRM system or IoT device).
2. Copy and transform the data with Azure Synapse Analytics.
3. Store the prepared data in an Azure Blob Storage.
4. Train the model with Azure Machine Learning.



![Diagram that shows data extracted, transformed with Azure Synapse Analytics, stored in a Storage Account, and served to Azure Machine Learning.](https://learn.microsoft.com/en-us/training/wwl-data-ai/design-data-ingestion-strategy-for-machine-learning-projects/media/04-01-pipeline.png)



# Design a machine learning model training solution

## +Identify machine learning tasks

1. **Classification**: Predict a categorical value.
2. **Regression**: Predict a numerical value.
3. **Time-series forecasting**: Predict future numerical values based on time-series data.
4. **Computer vision**: Classify images or detect objects in images.
5. **Natural language processing** (**NLP**): Extract insights from text.



## +Choose a service to train a machine learning model

**Azure Machine Learning** gives you many different options to train and manage your machine learning models. You can choose to work with the Studio for a UI-based experience, or manage your machine learning workloads with the Python SDK, or CLI for a code-first experience. Learn more about [Azure Machine Learning](https://learn.microsoft.com/en-us/azure/machine-learning/overview-what-is-azure-machine-learning).

**Azure AI Services** is a collection of prebuilt machine learning models you can use for common machine learning tasks such as object detection in images. The models are offered as an application programming interface (API), so you can easily integrate a model with your application. Some models can be customized with your own training data, saving time and resources to train a new model from scratch. Learn more about [Azure AI Services](https://learn.microsoft.com/en-us/azure/cognitive-services/what-are-cognitive-services).



Difference between services

- Use Azure AI Services whenever one of the customizable prebuilt models suits your requirements, to **save time and effort**.
- Use Azure Synapse Analytics or Azure Databricks if you want to **keep all data-related** (data engineering and data science) **projects within the same service**.
- Use Azure Synapse Analytics or Azure Databricks if you need **distributed compute** for working with large datasets (datasets are large when you experience capacity constraints with standard compute). You'll need to work with [PySpark](https://spark.apache.org/docs/latest/api/python) to use the distributed compute.
- Use Azure Machine Learning or Azure Databricks when you want **full control** over model training and management.
- Use Azure Machine Learning when **Python** is your preferred programming language.
- Use Azure Machine Learning when you want an **intuitive user interface** to manage your machine learning lifecycle.



## Decide between compute options

CPU vs GPU vs Spark

If you need **real-time predictions**, you need compute that is always available and able to return the results (almost) immediately. **Container** technologies like *Azure Container Instance* (ACI) and *Azure Kubernetes Service* (AKS) are ideal for such scenarios as they provide a lightweight infrastructure for your deployed model.

Alternatively, if you need **batch predictions**, you need compute that can handle a large workload. Ideally, you'd use a **compute cluster** that can score the data in *parallel* batches by using multiple nodes.



# Design a model deployment solution

To integrate the model, you need to deploy a model to an **endpoint**. Two options

- Get **real-time** predictions
  - If you want the model to score any new data as it comes in, you need predictions in real-time.
  - Real-time predictions are often needed when a model is used by an application such as a mobile app or a website.
- Get **batch** predictions
  - If you want the model to score new data in batches, and save the results as a file or in a database, you need batch predictions.
  - Imagine you're visualizing all historical sales data in a report

Whether you want real-time or batch predictions *doesn't necessarily depend on how often new data is collected*. Instead, it depends on how often and how quickly you need the predictions to be generated.



If you need **real-time predictions**, you need compute that is always available and able to return the results (almost) immediately. **Container** technologies like *Azure Container Instance* (ACI) and *Azure Kubernetes Service* (AKS) are ideal for such scenarios as they provide a lightweight infrastructure for your deployed model.

Alternatively, if you need **batch predictions**, you need compute that can handle a large workload. Ideally, you'd use a **compute cluster** that can score the data in *parallel* batches by using multiple nodes.



# Design a ML operations solution

Implementing MLOps helps you to make your machine learning workloads robust and reproducible.

- Convert the model training to a **robust** and **reproducible** pipeline.
- Test the code and the model in a **development** environment.
- Deploy the model in a **production** environment.
- **Automate** the end-to-end process.



## +Set up environments for development and production

**Environment** refers to a collection of resources. These resources are used to deploy an application, or with machine learning projects, to deploy a model.

A typical approach is to:

- Experiment with model training in the *development* environment.
- Move the best model to the *staging* or *pre-prod* environment to deploy and test the model.
- Finally release the model to the *production* environment to deploy the model so that end-users can consume it.



## +Design an MLOps architecture

- Store all data in an Azure Blob storage, managed by the data engineer.
- The infrastructure team creates all necessary Azure resources, like the Azure Machine Learning workspace.
- Data scientists focus on what they do best: developing and training the model (inner loop).
- Machine learning engineers deploy the trained models (outer loop).



1. **Setup**: Create all necessary Azure resources for the solution.
2. **Model development (inner loop)**: Explore and process the data to train and evaluate the model.
3. **Continuous integration**: Package and register the model.
4. **Model deployment (outer loop)**: Deploy the model.
5. **Continuous deployment**: Test the model and promote to production environment.
6. **Monitoring**: Monitor model and endpoint performance.



## +Design for retraining

Ideally, you should train models with **scripts** instead of notebooks. Scripts are better suited for automation. You can add **parameters** to a script



# Configure Azure ML workspace

To create an Azure Machine Learning service, you'll have to:

1. Get access to **Azure**, for example through the Azure portal.

2. Sign in to get access to an **Azure subscription**.

3. Create a **resource group** within your subscription.

4. Create an **Azure Machine Learning service** to create a workspace.

   - Use the user interface in the **Azure portal** to create an Azure Machine Learning service.
   - Create an **Azure Resource Manager** (**ARM**) template. [Learn how to use an ARM template to create a workspace](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-workspace-template?tabs=azcli%3Fazure-portal%3Dtrue).
   - Use the **Azure Command Line Interface** (**CLI**) with the Azure Machine Learning CLI extension. [Learn how to create the workspace with the CLI v2](https://learn.microsoft.com/en-us/training/modules/create-azure-machine-learning-resources-cli-v2/).
   - Use the **Azure Machine Learning Python SDK**.

   When a workspace is provisioned, Azure will automatically create other Azure resources within the same resource group to support the workspace:

   - **Azure Storage Account**: To store files and notebooks used in the workspace, and to store metadata of jobs and models.

   - **Azure Key Vault**: To securely manage secrets such as authentication keys and credentials used by the workspace.

   - **Application Insights**: To monitor predictive services in the workspace.

   - **Azure Container Registry**: Created when needed to store images for Azure Machine Learning environments.

![Diagram of hierarchy of Azure resources needed for the Azure Machine Learning workspace.](https://learn.microsoft.com/en-us/training/wwl-azure/explore-azure-machine-learning-workspace-resources-assets/media/overview-azure-resources.png)

## +**Role-based access control** (**RBAC**)

which you can configure in the **Access control** tab

- **Owner**: Gets full access to all resources, and can grant access to others using access control.
- **Contributor**: Gets full access to all resources, but can't grant access to others.
- **Reader**: Can only view the resource, but isn't allowed to make any changes.

Additionally, Azure Machine Learning has specific built-in roles you can use:

- **AzureML Data Scientist**: Can perform all actions within the workspace, <u>except for creating or deleting</u> compute resources, or editing the workspace settings.
- **AzureML Compute Operator**: Is allowed to create, change, and <u>manage access</u> the compute resources within a workspace.



### Summary of the Machine Learning Workflow

#### 1. **Environments**

- **Development Environment**: Initial setup where data scientists and engineers write and test code. Tools like Jupyter Notebooks, IDEs, and version control systems (e.g., Git) are used.
- **Staging Environment**: Intermediate stage for testing and validation before production. Includes CI/CD pipelines and testing frameworks.
- **Production Environment**: Final stage where models are deployed for real-time use. Uses services like Azure Kubernetes Service (AKS) or cloud environments for scalability and reliability.

#### 2. **Assets**

- **Definition**: Components such as datasets, models, and experiment results that are used and produced throughout the ML lifecycle.
- **Management**: Handled within the ML platform (e.g., Azure ML workspace), ensuring versioning and tracking for reproducibility.

#### 3. **Signature**

- **Python Function Signature**: Defines input parameters and types for Python functions, ensuring correct data handling.
- **Tensor Signature**: Specifies the shape and type of tensors for deep learning models, crucial for frameworks like TensorFlow and PyTorch.

#### 4. **Flavor**

- **Definition**: Specific configuration for a model based on the machine learning framework used (e.g., Scikit-learn, TensorFlow, PyTorch).
- **Purpose**: Ensures compatibility and standardization, facilitating model saving, loading, and deployment.

#### 5. **Models**

- **Training**: Models are trained using algorithms on prepared datasets.
- **Logging**: Models, along with their configurations and metrics, are logged for tracking and reproducibility.
- **Deployment**: Models are deployed to production environments using specified flavors, ensuring they can be served correctly.

#### 6. **Datastores**

- **Definition**: Abstractions over storage services for managing data access.
- Auto-Created Datastores:
  - `workspaceblobstore`: Azure Blob Storage for large datasets.
  - `workspacefilestore`: Azure File Storage for file-based data.
- **Common Additional Datastore**: Azure Data Lake Storage (ADLS) for handling large volumes of structured and unstructured data.

#### 7. **Artifacts**

- **Definition**: Files and outputs generated during the ML process, including logs, datasets, and trained models.
- **Management**: Stored and managed in the ML platform’s artifact storage, ensuring accessibility and versioning.





# Identify Azure Machine Learning resources

The resources in Azure Machine Learning include:

- *The workspace*
  - The **workspace** is the top-level resource for Azure Machine Learning. Data scientists need access to the workspace to train and track models, and to deploy the models to endpoints.
- *Compute resources*
- *Datastores*



## +Create and manage compute resources

- **Compute instances**: Similar to a virtual machine in the cloud, managed by the workspace. Ideal to use as a <u>development environment</u> to run (Jupyter) notebooks.
- **Compute clusters**: On-demand clusters of CPU or GPU compute nodes in the cloud, managed by the workspace. Ideal to use for <u>production workloads</u> as they automatically scale to your needs.
- **Kubernetes clusters**: Allows you to <u>create or attach</u> an Azure Kubernetes Service (AKS) cluster. Ideal to deploy trained machine learning models in production scenarios.
- **Attached computes**: Allows you to attach other Azure compute resources to the workspace, like Azure Databricks or Synapse Spark pools.
- **Serverless compute**: A fully managed, on-demand compute you can use for training jobs.



## +Create and manage datastores

- `workspaceartifactstore`: Connects to the `azureml` container of the Azure Storage account created with the workspace. Used to store compute and experiment logs when running jobs.
- `workspaceworkingdirectory`: Connects to the file share of the Azure Storage account created with the workspace used by the **Notebooks** section of the studio. Whenever you upload files or folders to access from a compute instance, it's uploaded to this file share.
- `workspaceblobstore`: Connects to the Blob Storage of the Azure Storage account created with the workspace. Specifically the `azureml-blobstore-...` container. Set as the default datastore, which means that whenever you create a data asset and upload data, it's stored in this container.
- `workspacefilestore`: Connects to the file share of the Azure Storage account created with the workspace. Specifically the `azureml-filestore-...` file share.



## + Identify Azure Machine Learning assets

Assets are created and used at various stages of a project and include:

- Models
  - pickle file (`.pkl` extension) for storage
  - when create a **model** in the workspace, specify the <u>*name* and *version*</u>
- Environments
  - Environments specify software packages, environment variables, and software settings to run scripts
- Data
  - You can use data assets to easily access data every time, without having to provide authentication 
  - <u>Path, name, and version</u>
- Components
  - Reuse code basically
  - you have to specify the <u>*name*, *version*, code, and *environment*</u>
  - You can use components when creating **pipelines**. A component therefore often represents a step in a pipeline



## +Train models in the **workspace**

To train models with the Azure Machine Learning workspace, you have several options:

- Use **Automated Machine Learning**.
  - Automated Machine Learning iterates through algorithms paired with feature selections to find the best performing model for your data
- Run a Jupyter notebook.
  - The **Notebooks** page in the studio allows you to edit and run Jupyter notebooks.
- Run a script as a job..
  - You can run a script as a **job** in Azure Machine Learning. When you submit a job to the workspace, all inputs and outputs will be stored in the workspace.

There are different types of jobs depending on how you want to execute a workload:

1. **Command**: Execute a single script.
2. **Sweep**: Perform hyperparameter tuning when executing a single script.
3. **Pipeline**: Run a pipeline consisting of multiple scripts or components.



# Explore developer tools for workspace interaction

## +Python SDK

After the Python SDK is installed, you'll need to connect to the workspace

To authenticate, you need the values to three necessary parameters:

- `subscription_id`: Your subscription ID.
- `resource_group`: The name of your resource group.
- `workspace_name`: The name of your workspace.

Next, you can define the authentication by using the following code:

![image-20240629135020498](C:\Users\bread\AppData\Roaming\Typora\typora-user-images\image-20240629135020498.png)

After defining the authentication, you need to call `MLClient` for the environment to connect to the workspace. You'll call `MLClient` anytime you want to create or update an asset or resource in the workspace.

```
returned_job = ml_client.create_or_update(job)
```



## +Explore the CLI (command-line interface)

The Azure CLI is commonly used by administrators and engineers to automate tasks in Azure.

The Azure CLI allows you to:

- Automate the creation and configuration of assets and resources to make it **repeatable**.
- Ensure **consistency** for assets and resources that must be replicated in multiple environments (for example, development, test, and production).
- Incorporate machine learning asset configuration into developer operations (**DevOps**) **workflows**, such as **continuous integration** and **continuous deployment** (**CI/CD**) pipelines.



## +Install the Azure Machine Learning extension

Use Azure Machine Learning extension to manage Azure Machine Learning resources using the Azure CLI.

You can install the Azure Machine Learning extension `ml` with the following command:

```
az extension add -n ml -y

az ml -h
```



# Make data available in Azure Machine Learning

## +Understand URIs

- `http(s)`: Use for data stores publicly or privately in an Azure Blob Storage or publicly available http(s) location.
- `abfs(s)`: Use for data stores in an Azure Data Lake Storage Gen 2.
- `azureml`: Use for data stored in a datastore.



## +Create a datastore

In Azure Machine Learning, **datastores** are abstractions for cloud data sources. They encapsulate the information needed to connect to data sources, and securely store this connection information so that you don’t have to code it in your scripts. (Datastores allow you to **easily connect** to storage services)

Azure Machine Learning supports the creation of datastores for multiple kinds of Azure data source, including:

- Azure Blob Storage
- Azure File Share
- Azure Data Lake (Gen 2)

Every workspace has **four** built-in datastores (two connecting to Azure Storage blob containers, and two connecting to Azure Storage file shares), which are used as system storages by Azure Machine Learning.

In most machine learning projects, you need to work with data sources of your own.

You can create a datastore through the **graphical user interface**, the Azure command-line interface (**CLI**), or the Python software development kit (**SDK**).

```
blob_datastore = AzureBlobDatastore(
    			name = "blob_example",
    			description = "Datastore pointing to a blob container",
    			account_name = "mytestblobstore",
    			container_name = "data-container",
    			credentials = AccountKeyConfiguration(
        			account_key="XXXxxxXXXxXXXXxxXXX"
    			),
)
ml_client.create_or_update(blob_datastore)
```



## +Create a data asset

To simplify getting access to the data you want to work with, you can use **data assets**.

*data assets are references to where the data is stored, how to get access, and any other relevant metadata*

- You can **share and reuse data** with other members of the team such that they don't need to remember file locations.
- You can **seamlessly access data** during model training (on any supported compute type) without worrying about connection strings or data paths.
- You can **version** the metadata of the data asset.

There are three main types of data assets you can use: 

1. **URI file**: Points to a specific file.
2. **URI folder**: Points to a folder.
3. **MLTable**: Points to a folder or file, and includes a schema to read as tabular data.



### Create a URI file data asset

- Local: `./<path>`
- Azure Blob Storage: `wasbs://<account_name>.blob.core.windows.net/<container_name>/<folder>/<file>`
- Azure Data Lake Storage (Gen 2): `abfss://<file_system>@<account_name>.dfs.core.windows.net/<folder>/<file>`
- Datastore: `azureml://datastores/<datastore_name>/paths/<folder>/<file>`

```
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes

my_path = '<supported-path>'

my_data = Data(
    path=my_path,
    type=AssetTypes.URI_FILE,
    description="<description>",
    name="<name>",
    version="<version>"
)

ml_client.data.create_or_update(my_data)

#READ INPUT DATA
parser = argparse.ArgumentParser()
parser.add_argument("--input_data", type=str)
args = parser.parse_args()

df = pd.read_csv(args.input_data)
print(df.head(10))
```



### Create a MLTable data asset

For certain features in Azure Machine Learning, like *Automated Machine Learning*, you need to use a MLTable data asset, as Azure Machine Learning needs to know how to read the data.

To define the schema, you can include a **MLTable file** in the same folder as the data you want to read. The MLTable file includes the path pointing to the data you want to read, and how to read the data:

```yml
type: mltable

paths:
  - pattern: ./*.txt
transformations:
  - read_delimited:
      delimiter: ','
      encoding: ascii
      header: all_files_same_headers
```





# Work with compute targets in Azure Machine Learning



## +Types of compute

- **Compute instance**: Behaves similarly to a virtual machine and is primarily used to run notebooks. It's ideal for *experimentation*.
  - To use a compute instance, you need an application that can host notebooks. The easiest option to work with the compute instance is through the integrated notebooks experience in the Azure Machine Learning studio.
- **Compute clusters**: Multi-node clusters of virtual machines that automatically scale up or down to meet demand. A cost-effective way to run scripts that need to process large volumes of data. Clusters also allow you to use parallel processing to distribute the workload and reduce the time it takes to run a script.
  - Running a pipeline job you built in the Designer.
  - Running an Automated Machine Learning job.
  - Running a script as a job.
- **Kubernetes clusters**: Cluster based on Kubernetes technology, giving you more control over how the compute is configured and managed. You can attach your self-managed Azure Kubernetes (AKS) cluster for cloud compute, or an Arc Kubernetes cluster for on-premises workloads.
- **Attached compute**: Allows you to attach existing compute like Azure virtual machines or Azure Databricks clusters to your workspace.
- **Serverless compute**: A fully managed, on-demand compute you can use for training jobs.



### Experimentation

Many data scientists are familiar with running notebooks on their local device. A cloud alternative managed by Azure Machine Learning is a *compute instance*. Alternatively, you can also opt for *Spark serverless compute* to run Spark code in notebooks, if you want to make use of Spark's distributed compute power.

### Production

When training models with scripts, you want an on-demand compute target. A *compute cluster* automatically scales up when the script(s) need to be executed, and scales down when the script finishes executing

### **Batch predictions**

For batch predictions, you can run a pipeline job in Azure Machine Learning. Compute targets like compute clusters and Azure Machine Learning's serverless compute are ideal for pipeline jobs as they're on-demand and scalable.

### Real-time predictions

When you want real-time predictions, you need a type of compute that is running continuously. Containers are ideal for real-time deployments. Alternatively, you can attach Kubernetes clusters to manage the necessary compute to generate real-time predictions.



## +Create and use a compute

Compute instance:

```
from azure.ai.ml.entities import ComputeInstance

ci_basic_name = "basic-ci-12345"
ci_basic = ComputeInstance(
    name=ci_basic_name, 
    size="STANDARD_DS3_v2"
)
ml_client.begin_create_or_update(ci_basic).result()
```

Compute cluster:

```
from azure.ai.ml.entities import AmlCompute

cluster_basic = AmlCompute(
    name="cpu-cluster",
    type="amlcompute",
    size="STANDARD_DS3_v2",
    location="westus",
    min_instances=0,
    max_instances=2,
    idle_time_before_scale_down=120,
    tier="low_priority",
)
ml_client.begin_create_or_update(cluster_basic).result()
```

- `size`: Specifies the *virtual machine type* of each node within the compute cluster. Based on the [sizes for virtual machines in Azure](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes). Next to size, you can also specify whether you want to use CPUs or GPUs.
- `max_instances`: Specifies the *maximum number of nodes* your compute cluster can scale out to. The number of parallel workloads your compute cluster can handle is analogous to the number of nodes your cluster can scale to.
- `tier`: Specifies whether your virtual machines are *low priority* or *dedicated*. Setting to low priority can lower costs as you're not guaranteed availability.



# Work with environments in Azure ML

**Environments** list and store the necessary packages that you can reuse across compute targets.

Azure Machine Learning builds the environment on the **Azure Container registry** associated with the workspace. When you create an Azure Machine Learning workspace, **curated** environments are automatically created and made available to you.

For example, to list the environments using the Python SDK:

```
envs = ml_client.environments.list()
for env in envs:
    print(env.name)
```

To review the details of a specific environment, you can retrieve an environment by its registered name:

```
env = ml_client.environments.get(name="my-environment", version="1")
print(env)
```

Curated environments use the prefix **AzureML-**



## +Explore and use curated environments

Most commonly, you use environments when you want to run a script as a (**command**) **job**.

```
from azure.ai.ml import command

# configure job
job = command(
    code="./src",
    command="python train.py",
    environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
    compute="aml-cluster",
    display_name="train-with-curated-environment",
    experiment_name="train-with-curated-environment"
)

# submit job
returned_job = ml_client.create_or_update(job)
```





# Auto ML

An approach of training the best machine learning model which requires no coding and minimum skills in machine learning

Before you can run an automated machine learning, you need to create a **data asset** in Azure Machine Learning. In order for AutoML to understand how to read the data, you need to create a **MLTable** data asset that includes the schema of the data.

```
from azure.ai.ml.constants import AssetTypes
from azure.ai.ml import Input

my_training_data_input = Input(type=AssetTypes.MLTABLE, path="azureml:input-data-automl:1")
```

You can choose to have AutoML apply *preprocessing transformations*, such as:

- Missing value imputation to eliminate nulls in the training dataset.
- Categorical encoding to convert categorical features to numeric indicators.
- Dropping high-cardinality features, such as record IDs.
- Feature engineering (for example, deriving individual date parts from DateTime features)

By default, AutoML will perform featurization on your data. You can disable it if you don't want the data to be transformed.

If you do want to make use of the integrated featurization function, you can customize it. For example, you can specify which imputation method should be used for a specific feature.



## +Run a Automated ML experiment

AutoML will choose from a list of classification algorithms:

- Logistic Regression
- Light Gradient Boosting Machine (GBM)
- Decision Tree
- Random Forest
- Naive Bayes
- Linear Support Vector Machine (SVM)
- XGBoost
- And others...

You can choose to block individual algorithms from being selected; 

```
classification_job = automl.classification(
    compute="aml-cluster",
    experiment_name="auto-ml-class-dev",
    training_data=my_training_data_input,
    target_column_name="Diabetic",
    primary_metric="accuracy",
    n_cross_validations=5,
    enable_model_explainability=True
)
```



One of the most important settings you must specify is the **primary_metric**. The primary metric is the target performance metric for which the optimal model will be determined. Azure Machine Learning supports a set of named metrics for each type of task.

```
from azure.ai.ml.automl import ClassificationPrimaryMetrics
 
list(ClassificationPrimaryMetrics)
```



There are several options to set limits to an AutoML experiment:

- `timeout_minutes`: Number of minutes after which the complete AutoML experiment is terminated.
- `trial_timeout_minutes`: Maximum number of minutes one trial can take.
- `max_trials`: Maximum number of trials, or models that will be trained.
- `enable_early_termination`: Whether to end the experiment if the score isn't improving in the short term.



You can submit an AutoML job with the following code:

```
returned_job = ml_client.jobs.create_or_update(
    classification_job
) 

#You can monitor AutoML job with studio_url
aml_url = returned_job.studio_url
print("Monitor your job at", aml_url)
```

# MLflow for notebooks

## +Logging

**MLflow** is an open-source library for tracking and managing your machine learning experiments. In particular, **MLflow Tracking** is a component of MLflow that logs everything about the model you're training, such as **parameters**, **metrics**, and **artifacts**.

```
pip show mlflow
pip show azureml-mlflow


mlflow.set_experiment(experiment_name="heart-condition-classifier")

with mlflow.start_run():
    mlflow.xgboost.autolog()

    model = XGBClassifier(use_label_encoder=False, eval_metric="logloss")
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
```



### Autologging

MLflow supports automatic logging for popular machine learning libraries. If you're using a library that is supported by autolog, then MLflow tells the framework you're using to log all the metrics, parameters, artifacts, and models that the framework considers relevant.

You can turn on autologging by using the `autolog` method for the framework you're using. For example, to enable autologging for XGBoost models you can use `mlflow.xgboost.autolog()`.



When the job has completed, you can review all logged metrics in the studio.

![Screenshot of overview page of MLflow experiment with autologging in Azure Machine Learning studio.](https://learn.microsoft.com/en-us/training/wwl-azure/track-model-training-jupyter-notebooks-mlflow/media/autolog-results.png)

### Custom logging

Manually logging models is helpful when you want to log supplementary or custom information that isn't logged through autologging.

Common functions used with custom logging are:

- `mlflow.log_param()`: Logs a single key-value parameter. Use this function for an input parameter you want to log.
- `mlflow.log_metric()`: Logs a single key-value metric. Value must be a number. Use this function for any output you want to store with the run.
- `mlflow.log_artifact()`: Logs a file. Use this function for any plot you want to log, save as image file first.
- `mlflow.log_model()`: Logs a model. Use this function to create an MLflow model, which may include a custom signature, environment, and input examples.



# Run a training script as a command job

Scripts are ideal for **testing and automation** in your production environment. To create a production-ready script, you'll need to:

- Remove nonessential code.
  - you want to avoid executing anything nonessential to reduce cost and compute time.
- Refactor your code into functions.
  - you want the code to be easy to read so that anyone can maintain it
- Test your script in the terminal.



## +Run a script as a command job

To run a script as a command job, you'll need to configure and submit the job.

To configure a command job with the Python SDK (v2), you'll use the `command` function. To run a script, you'll need to specify values for the following parameters:

- `code`: The folder that includes the script to run.
- `command`: Specifies which file to run.
- `environment`: The necessary packages to be installed on the compute before running the command.
- `compute`: The compute to use to run the command.
- `display_name`: The name of the individual job.
- `experiment_name`: The name of the experiment the job belongs to.

```
from azure.ai.ml import command

# configure job
job = command(
    code="./src",
    command="python train.py",
    environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
    compute="aml-cluster",
    display_name="train-model",
    experiment_name="train-classification-model"
    )
    
# submit job
returned_job = ml_client.create_or_update(job)
```



To use parameters in a script, you must use a library such as `argparse` to read arguments passed to the script and assign them to variables.

```
def parse_args():
    # setup arg parser
    parser = argparse.ArgumentParser()

    # add arguments
    parser.add_argument("--training_data", dest='training_data',
                        type=str)

    # parse args
    args = parser.parse_args()

    # return args
    return args
    
# command to pass parameter to script    
python train.py --training_data diabetes.csv
```



# Perform hyperparameter tuning with  Azure Machine Learning

Data scientists refer to the values determined from the training features as *parameters*, so a different term is required for values that are used to configure training behavior but which are ***not*** derived from the training data - hence the term *hyperparameter*.

**Hyperparameter tuning** is accomplished by training the multiple models, using the same algorithm and training data but different hyperparameter values. 

In Azure Machine Learning, you can tune hyperparameters by submitting a script as a **sweep job**. A sweep job will run a **trial** for each hyperparameter combination to be tested.



## +Search space

The set of hyperparameter values tried during hyperparameter tuning is known as the **search space**. 



### Discrete hyperparameters

Some hyperparameters require *discrete* values - in other words, you must select the value from a particular *finite* set of possibilities. You can define a search space for a discrete parameter using a **Choice** from a list of explicit values, which you can define as a Python **list** (`Choice(values=[10,20,30])`), a **range** (`Choice(values=range(1,10))`), or an arbitrary set of comma-separated values (`Choice(values=(30,50,100))`)

You can also select discrete values from any of the following discrete distributions:

- `QUniform(min_value, max_value, q)`: Returns a value like round(Uniform(min_value, max_value) / q) * q
- `QLogUniform(min_value, max_value, q)`: Returns a value like round(exp(Uniform(min_value, max_value)) / q) * q
- `QNormal(mu, sigma, q)`: Returns a value like round(Normal(mu, sigma) / q) * q
- `QLogNormal(mu, sigma, q)`: Returns a value like round(exp(Normal(mu, sigma)) / q) * q

### Continuous hyperparameters

Some hyperparameters are *continuous* - in other words you can use any value along a scale, resulting in an *infinite* number of possibilities. To define a search space for these kinds of value, you can use any of the following distribution types:

- `Uniform(min_value, max_value)`: Returns a value uniformly distributed between min_value and max_value
- `LogUniform(min_value, max_value)`: Returns a value drawn according to exp(Uniform(min_value, max_value)) so that the logarithm of the return value is uniformly distributed
- `Normal(mu, sigma)`: Returns a real value that's normally distributed with mean mu and standard deviation sigma
- `LogNormal(mu, sigma)`: Returns a value drawn according to exp(Normal(mu, sigma)) so that the logarithm of the return value is normally distributed



To define a search space for hyperparameter tuning, create a dictionary with the appropriate parameter expression for each named hyperparameter.

For example, the following search space indicates that the `batch_size` hyperparameter can have the value 16, 32, or 64, and the `learning_rate` hyperparameter can have any value from a normal distribution with a mean of 10 and a standard deviation of 3.

```
from azure.ai.ml.sweep import Choice, Normal

command_job_for_sweep = job(
    batch_size=Choice(values=[16, 32, 64]),    
    learning_rate=Normal(mu=10, sigma=3),
)

sweep_job = command_job_for_sweep.sweep(
    sampling_algorithm = "random",
    ...
)
```



## +Configure a sampling method

There are three main sampling methods available in Azure Machine Learning:

- **Grid sampling**: Tries every possible combination.

- **Random sampling**

  : Randomly chooses values from the search space.

  - **Sobol**: Adds a seed to random sampling to make the results reproducible.

- **Bayesian sampling**: Chooses new values based on previous results.



## +Configure an early termination policy

There are two main parameters when you choose to use an early termination policy:

- `evaluation_interval`: Specifies at which interval you want the policy to be evaluated. Every time the primary metric is logged for a trial counts as an interval.
- `delay_evaluation`: Specifies when to start evaluating the policy. This parameter allows for at least a minimum of trials to complete without an early termination policy affecting them.

New models may continue to perform only slightly better than previous models. To determine the extent to which a model should perform better than previous trials, there are three options for early termination:

- **Bandit policy**: Uses a `slack_factor` (relative) or `slack_amount`(absolute). Any new model must perform within the slack range of the best performing model.
- **Median stopping policy**: Uses the median of the averages of the primary metric. Any new model must perform better than the median.
- **Truncation selection policy**: Uses a `truncation_percentage`, which is the percentage of lowest performing trials. Any new model must perform better than the lowest performing trials.
  - In other words, if the fifth trial is **not** the worst performing model so far, the sweep job will continue. If the fifth trial has the lowest accuracy score of all trials so far, the sweep job will stop.



# Component

**Components** allow you to create reusable scripts that can easily be shared across users within the same Azure Machine Learning workspace.

A component consists of three parts:

- **Metadata**: Includes the component's name, version, etc.
- **Interface**: Includes the expected input parameters (like a dataset or hyperparameter) and expected output (like metrics and artifacts).
- **Command, code and environment**: Specifies how to run the code.



# Create a pipeline

In Azure Machine Learning, a **pipeline** is a workflow of machine learning tasks in which each task is defined as a **component**.

A pipeline can be executed as a process by running the pipeline as a **pipeline job**. Each component is executed as a **child job** as part of the overall pipeline job.

![Diagram of pipeline structure including all inputs and outputs.](https://learn.microsoft.com/en-us/training/wwl-azure/run-pipelines-azure-machine-learning/media/pipeline-overview.png)

To schedule a pipeline job, you'll use the `JobSchedule` class to associate a schedule to a pipeline job.

There are various ways to create a schedule. A simple approach is to create a time-based schedule using the `RecurrenceTrigger` class with the following parameters:

- `frequency`: Unit of time to describe how often the schedule fires. Value can be either `minute`, `hour`, `day`, `week`, or `month`.
- `interval`: Number of frequency units to describe how often the schedule fires. Value needs to be an integer.

To schedule a pipeline, you'll need `pipeline_job` to represent the pipeline you've built:

```
from azure.ai.ml.entities import RecurrenceTrigger

schedule_name = "run_every_minute"

recurrence_trigger = RecurrenceTrigger(
    frequency="minute",
    interval=1,
)

job_schedule = JobSchedule(
    name=schedule_name, trigger=recurrence_trigger, create_job=pipeline_job
)

job_schedule = ml_client.schedules.begin_create_or_update(
    schedule=job_schedule
).result()
```





# Log models with MLflow

MLflow standardizes the packaging of models, which means that an MLflow model can easily be imported or exported across different workflows.

If you want to **export** the model to another workspace used for production, you can use an MLflow model to easily do so.

When you register the model, an `MLmodel` file is created in that directory. The `MLmodel` file contains the model's metadata, which allows for model traceability. (this is included in the output folder created in autologging)



The model is logged when the `.fit()` method is called. 

- Keras: `mlflow.keras.autolog`
- Scikit-learn: `mlflow.sklearn.autolog()`
- LightGBM: `mlflow.lightgbm.autolog`
- XGBoost: `mlflow.xgboost.autolog`
- TensorFlow: `mlflow.tensorflow.autolog`
- PyTorch: `mlflow.pytorch.autolog`
- ONNX: `mlflow.onnx.autolog`

When you want to have more control over how the model is logged, you can use `autolog` (for your parameters, metrics, and other artifacts), and set `log_models=False`.



## +Explore the MLmodel file format

The `MLmodel` file may include:

- `artifact_path`: During the training job, the model is logged to this path.
- `flavor`: The machine learning library with which the model was created.
- `model_uuid`: The unique identifier of the registered model.
- `run_id`: The unique identifier of job run during which the model was created.
- Signature: Specifies the schema of the model's inputs and outputs:
  - `inputs`: Valid input to the model. For example, a subset of the training dataset.
  - `outputs`: Valid model output. For example, model predictions for the input dataset.
    - **Column-based**: used for tabular data with a `pandas.Dataframe` as inputs.
    - **Tensor-based**: used for n-dimensional arrays or tensors (often used for unstructured data like text or images), with `numpy.ndarray` as inputs.



## +MLflow models

There are three types of models you can register:

- **MLflow**: Model trained and tracked with MLflow. Recommended for standard use cases.
- **Custom**: Model type with a custom standard not currently supported by Azure Machine Learning.
- **Triton**: Model type for deep learning workloads. Commonly used for TensorFlow and PyTorch model deployments.



To train model, you can submit a training script as a command job 

![image-20240701192853604](C:\Users\bread\AppData\Roaming\Typora\typora-user-images\image-20240701192853604.png)

Once the job is completed and the model is trained, use the job name to find the job run and register the model from its outputs.

![image-20240701193405719](C:\Users\bread\AppData\Roaming\Typora\typora-user-images\image-20240701193405719.png)



# Create the Responsible AI dashboard

To create a Responsible AI (RAI) dashboard, you need to create a **pipeline** by using the built-in components. The pipeline should:

1. Start with the `RAI Insights dashboard constructor`.
2. Include one of the **RAI tool components**.
3. End with `Gather RAI Insights dashboard` to collect all insights into one dashboard.
4. *Optionally* you can also add the `Gather RAI Insights score card` at the end of your pipeline.



The available tool components and the insights you can use are:

- `Add Explanation to RAI Insights dashboard`: Interpret models by generating explanations. Explanations show how much features influence the prediction.
- `Add Causal to RAI Insights dashboard`: Use historical data to view the causal effects of features on outcomes.
- `Add Counterfactuals to RAI Insights dashboard`: Explore how a change in input would change the model's output.
- `Add Error Analysis to RAI Insights dashboard`: Explore the distribution of your data and identify erroneous subgroups of data.



After you've trained and registered a model in the Azure Machine Learning workspace, you can create the Responsible AI dashboard in three ways:

- Using the Command Line Interface (CLI) extension for Azure Machine Learning.
- Using the Python Software Development Kit (SDK).
  - Register the training and test datasets as MLtable data assets.
  - Register the model.
  - Retrieve the built-in components you want to use.
  - Build the pipeline.
  - Run the pipeline.
- Using the Azure Machine Learning studio for a no-code experience.



# Evaluate the Responsible AI dashboard

The output of each component you added to the pipeline is reflected in the dashboard. Depending on the components you selected, you can find the following insights in your Responsible AI dashboard:

### Error analysis

When you include error analysis, there are two types of visuals you can explore in the Responsible AI dashboard:

- **Error tree map**: Allows you to explore which combination of subgroups results in the model making more false predictions.

  ![Screenshot of error tree of diabetes classification model.](https://learn.microsoft.com/en-us/training/wwl-azure/manage-compare-models-azure-machine-learning/media/error-tree.png)

- **Error heat map**: Presents a grid overview of a model's errors over the scale of one or two features.

![Screenshot of error heat map of diabetes classification model.](https://learn.microsoft.com/en-us/training/wwl-azure/manage-compare-models-azure-machine-learning/media/error-map.png)

### Explanations

- **Aggregate feature importance**: Shows how each feature in the test data influences the model's predictions *overall*.

![Screenshot of aggregate feature importance for diabetes dataset.](https://learn.microsoft.com/en-us/training/wwl-azure/manage-compare-models-azure-machine-learning/media/aggregate-feature.png)

- **Individual feature importance**: Shows how each feature impacts an *individual* prediction.

![Screenshot of individual feature importance for one row in the diabetes dataset.](https://learn.microsoft.com/en-us/training/wwl-azure/manage-compare-models-azure-machine-learning/media/individual-feature.png)

### Counterfactuals

To explore how the model's output would change based on a change in the input, you can use **counterfactuals**.



### Causal analysis

**Causal analysis** uses statistical techniques to estimate the average effect of a feature on a desired prediction. It analyzes how certain interventions or treatments may result in a better outcome, across a population or for a specific individual.

There are three available tabs in the Responsible AI dashboard when including causal analysis:

- **Aggregate causal effects**: Shows the average causal effects for predefined treatment features (the features you want to change to optimize the model's predictions).
- **Individual causal effects**: Shows individual data points and allows you to change the treatment features to explore their influence on the prediction.
- **Treatment policy**: Shows which parts of your data points benefit most from a treatment. 





# Explore managed online endpoints

To get real-time predictions, you can deploy a model to an endpoint. An **endpoint** is an HTTPS endpoint to which you can send data, and which will return a response (almost) immediately.

Any data you send to the endpoint will serve as the input for the scoring script hosted on the endpoint. The scoring script loads the trained model to *predict the label for the new input data*, which is also called **inferencing**. 



Within Azure Machine Learning, there are two types of online endpoints:

- **Managed online endpoints**: Azure Machine Learning manages all the underlying infrastructure.
- **Kubernetes online endpoints**: Users manage the Kubernetes cluster which provides the necessary infrastructure.



To deploy your model to a managed online endpoint, you need to specify four things:

- **Model assets** like the model pickle file, or a **registered model** in the Azure Machine Learning workspace.
- **Scoring script** that loads the model.
- **Environment** which lists all necessary packages that need to be installed on the compute of the endpoint.
- **Compute configuration** including the needed **compute size** and **scale settings** to ensure you can handle the amount of requests the endpoint will receive.



To create an online endpoint, you'll use the `ManagedOnlineEndpoint` class, which requires the following parameters:

- `name`: Name of the endpoint. Must be unique in the Azure region.
- `auth_mode`: Use `key` for key-based authentication. Use `aml_token` for Azure Machine Learning token-based authentication.

```
from azure.ai.ml.entities import ManagedOnlineEndpoint

# create an online endpoint
endpoint = ManagedOnlineEndpoint(
    name="endpoint-example",
    description="Online endpoint",
    auth_mode="key",
)

ml_client.begin_create_or_update(endpoint).result()
```



## +Endpoint Deployment

One endpoint can have multiple deployments. One approach is the **blue/green deployment**.

When a request is sent to the endpoint, 90% of the traffic can go to the blue deployment, and 10% of the traffic can go to the *green deployment*. With two versions of the model deployed on the same endpoint, you can easily test the model. 

After testing, you can also seamlessly transition to the new version of the model by redirecting 90% of the traffic to the green deployment. If it turns out that the new version doesn't perform better, you can easily roll back to the first version of the model by redirecting most of the traffic back to the blue deployment.





## +Deploy your MLflow model to a managed online endpoint

The easiest way to deploy a model to an online endpoint is to use an **MLflow** model and deploy it to a *managed* online endpoint.

To deploy an MLflow model, you must have model files stored on a local path or with a registered model. You can log model files when training a model by using MLflow tracking.

Next to the model, you also need to specify the compute configuration for the deployment:

- `instance_type`: Virtual machine (VM) size to use. [Review the list of supported sizes](https://learn.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list).
- `instance_count`: Number of instances to use.

```
from azure.ai.ml.entities import Model, ManagedOnlineDeployment
from azure.ai.ml.constants import AssetTypes

# create a blue deployment
model = Model(
    path="./model",
    type=AssetTypes.MLFLOW_MODEL,
    description="my sample mlflow model",
)

blue_deployment = ManagedOnlineDeployment(
    name="blue",
    endpoint_name="endpoint-example",
    model=model,
    instance_type="Standard_F4s_v2",
    instance_count=1,
)

ml_client.online_deployments.begin_create_or_update(blue_deployment).result()
blue_deployment.traffic = {"blue": 100}
ml_client.begin_create_or_update(blue_deployment).result()
```



## +Deploy a NORMAL model to a managed online endpoint

You can choose to deploy a model to a managed online endpoint <u>without using the MLflow model format</u>. To deploy a model, you'll need to create the scoring script and define the environment necessary during inferencing.

To deploy a model, you must have:

- Model files stored on local path or registered model.
- A scoring script.
- An execution environment.

The model files can be logged and stored when you train a model.



The scoring script needs to include two functions:

- `init()`: Called when the service is initialized.
- `run()`: Called when new data is submitted to the service.

```
import json
import joblib
import numpy as np
import os

# called when the deployment is created or updated
def init():
    global model
    # get the path to the registered model file and load it
    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pkl')
    model = joblib.load(model_path)

# called when a request is received
def run(raw_data):
    # get the input data as a numpy array
    data = np.array(json.loads(raw_data)['data'])
    # get a prediction from the model
    predictions = model.predict(data)
    # return the predictions as any JSON serializable format
    return predictions.tolist()
```

To create an environment using a base Docker image, you can define the Conda dependencies in a `conda.yml` file:

```
name: basic-env-cpu
channels:
  - conda-forge
dependencies:
  - python=3.7
  - scikit-learn
  - pandas
  - numpy
  - matplotlib
```

Then, to create the environment, run the following code:

```
from azure.ai.ml.entities import Environment

env = Environment(
    image="mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04",
    conda_file="./src/conda.yml",
    name="deployment-environment",
    description="Environment created from a Docker image plus Conda environment.",
)
ml_client.environments.create_or_update(env)
```

```
from azure.ai.ml.entities import ManagedOnlineDeployment, CodeConfiguration

model = Model(path="./model",

blue_deployment = ManagedOnlineDeployment(
    name="blue",
    endpoint_name="endpoint-example",
    model=model,
    environment="deployment-environment",
    code_configuration=CodeConfiguration(
        code="./src", scoring_script="score.py"
    ),
    instance_type="Standard_DS2_v2",
    instance_count=1,
)

ml_client.online_deployments.begin_create_or_update(blue_deployment).result()
```



```
endpoint.traffic = {"blue": 100}
ml_client.begin_create_or_update(endpoint).result()

ml_client.online_endpoints.begin_delete(name="endpoint-example")
```



## +Test managed online endpoints

You can list all endpoints in the Azure Machine Learning studio, by navigating to the **Endpoints** page. In the **Real-time endpoints** tab, all endpoints are shown.



For testing, you can also use the Azure Machine Learning Python SDK to invoke an endpoint.

```
response = ml_client.online_endpoints.invoke(
    endpoint_name=online_endpoint_name,
    deployment_name="blue",
    request_file="sample-data.json",
)

if response[1]=='1':
    print("Yes")
else:
    print ("No")
```



# Deploy a model to a batch endpoint

In many production scenarios, long-running tasks that deal with large amounts of data are performed as **batch** operations. In machine learning, ***batch inferencing*** is used to asynchronously apply a predictive model to multiple cases and write the results to a file or database.

To get batch predictions, you can deploy a model to an endpoint. An **endpoint** is an HTTPS endpoint that you can call to trigger a batch scoring job.

Whenever the endpoint is invoked, a batch scoring job is submitted to the Azure Machine Learning workspace. The job typically uses a **compute cluster** to score multiple inputs. The results can be stored in a datastore, connected to the Azure Machine Learning workspace.

```
endpoint = BatchEndpoint(
    name="endpoint-example",
    description="A batch endpoint",
)

ml_client.batch_endpoints.begin_create_or_update(endpoint)
```



## +Deploy a model to a batch endpoint

You can deploy multiple models to a batch endpoint. Whenever you call the batch endpoint, which triggers a batch scoring job, the **default deployment** will be used unless specified otherwise.



## +Use compute clusters for batch deployments

The ideal compute to use for batch deployments is the Azure Machine Learning compute cluster. If you want the batch scoring job to process the new data in parallel batches, you need to provision a compute cluster with more than one maximum instances.

<u>To create a compute cluster, you can use the `AMLCompute` class.</u>

```
from azure.ai.ml.entities import AmlCompute

cpu_cluster = AmlCompute(
    name="aml-cluster",
    type="amlcompute",
    size="STANDARD_DS11_V2",
    min_instances=0,
    max_instances=4,
    idle_time_before_scale_down=120,
    tier="Dedicated",
)

cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)
```



# Deploy your MLflow model to a batch endpoint

To avoid needed a scoring script and environment, an MLflow model needs to be registered in the Azure Machine Learning workspace before you can deploy it to a batch endpoint.

To register an MLflow model, you'll use the `Model` class, while specifying the model type to be `MLFLOW_MODEL`. To register the model with the Python SDK, you can use the following code:

```
from azure.ai.ml.entities import Model
from azure.ai.ml.constants import AssetTypes

model_name = 'mlflow-model'
model = ml_client.models.create_or_update(
    Model(name=model_name, path='./model', type=AssetTypes.MLFLOW_MODEL)
)
```



To deploy an MLflow model to a batch endpoint, you'll use the `BatchDeployment` class.

When you configure the model deployment, you can specify:

- `instance_count`: Count of compute nodes to use for generating predictions.
- `max_concurrency_per_instance`: Maximum number of parallel scoring script runs per compute node.
- `mini_batch_size`: Number of files passed per scoring script run.
- `output_action`: What to do with the predictions: `summary_only` or `append_row`.
- `output_file_name`: File to which predictions will be appended, if you choose `append_row` for `output_action`.

```
from azure.ai.ml.entities import BatchDeployment, BatchRetrySettings
from azure.ai.ml.constants import BatchDeploymentOutputAction

deployment = BatchDeployment(
    name="forecast-mlflow",
    description="A sales forecaster",
    endpoint_name=endpoint.name,
    model=model,
    compute="aml-cluster",
    instance_count=2,
    max_concurrency_per_instance=2,
    mini_batch_size=2,
    output_action=BatchDeploymentOutputAction.APPEND_ROW,
    output_file_name="predictions.csv",
    retry_settings=BatchRetrySettings(max_retries=3, timeout=300),
    logging_level="info",
)
ml_client.batch_deployments.begin_create_or_update(deployment)
```

