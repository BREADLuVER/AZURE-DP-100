# Design a machine learning solution

## +Design a data ingestion strategy for ML projects

Extract data from a source and make it available to the Azure service

Extract, Transform, and Load **(*ETL*)**

Before being able to design the ETL or ELT process, you’ll need to identify your ***data source*** and ***data format**.*

Identify data source (storage)

- Customer Relationship Management (**CRM**) system
- transactional database like an **SQL database**
- generated by an Internet of Things (**IoT**) device.

Identify data format (storage)

- **Tabular** or **structured** data: All data has the same fields or properties, which are defined in a schema (Excel or CSV file)

- **Semi-structured** data: Instead, each data point is represented by a collection of *key-value pairs*. (loT generate a JSON object)
- **Unstructured** data: Files that don't adhere to any rules when it comes to structure. For example, documents, images, audio, and video files



## +Choose how to serve data to ML workflows

It's best practice to separate compute from storage

When you use **Azure Machine Learning**, **Azure Databricks**, or **Azure Synapse Analytics** for *model training*, there are three common options for storing data, which are easily connected to all three services:

- **Azure Blob Storage**: Cheapest option for storing data as *unstructured* data. Ideal for storing files like images, text, and JSON. Often also used to store data as CSV files, as data scientists prefer working with CSV files.
- **Azure Data Lake Storage (Gen 2)**: A more advanced version of the Azure Blob Storage. Also stores files like CSV files and images as *unstructured* data. A data lake also implements a hierarchical namespace, which means it’s easier to give someone access to a specific file or folder. Storage capacity is virtually limitless so ideal for storing large data.
- **Azure SQL Database**: Stores data as *structured* data. Data is read as a table and schema is defined when a table in the database is created. Ideal for data that doesn’t change over time.



## +Design a data ingestion pipeline

### Azure Synapse Analytics (**Azure Synapse Pipelines**)

- copy data from one source to a data store using connectors
- use **mapping data flow** or use a language like SQL, Python, or R for data transformation
- choose between different types of compute that can handle large data transformations at scale: server-less SQL pools, dedicated SQL pools, or Spark pools.

### Azure Data bricks

Azure Data-bricks allows you to define your pipelines in a notebook or use code-first tools like SQL, Python, or R to create your pipelines, which you can schedule to run. uses Spark clusters

### Azure Machine Learning

Create a pipeline with the Designer, or by creating a collection of scripts. Can also extract, transform, and store the data in preparation for training. Uses clusters 

Whenever you want to perform *all tasks within the same tool*, creating and scheduling an Azure Machine Learning pipeline to run with the on-demand compute cluster may best suit your needs.



### Design a data ingestion solution

data ingestion steps:

1. Extract raw data from its source (like a CRM system or IoT device).
2. Copy and transform the data with Azure Synapse Analytics.
3. Store the prepared data in an Azure Blob Storage.
4. Train the model with Azure Machine Learning.



![Diagram that shows data extracted, transformed with Azure Synapse Analytics, stored in a Storage Account, and served to Azure Machine Learning.](https://learn.microsoft.com/en-us/training/wwl-data-ai/design-data-ingestion-strategy-for-machine-learning-projects/media/04-01-pipeline.png)



# Design a machine learning model training solution

## +Identify machine learning tasks

1. **Classification**: Predict a categorical value.
2. **Regression**: Predict a numerical value.
3. **Time-series forecasting**: Predict future numerical values based on time-series data.
4. **Computer vision**: Classify images or detect objects in images.
5. **Natural language processing** (**NLP**): Extract insights from text.



## +Choose a service to train a machine learning model

**Azure Machine Learning** gives you many different options to train and manage your machine learning models. You can choose to work with the Studio for a UI-based experience, or manage your machine learning workloads with the Python SDK, or CLI for a code-first experience. Learn more about [Azure Machine Learning](https://learn.microsoft.com/en-us/azure/machine-learning/overview-what-is-azure-machine-learning).

**Azure AI Services** is a collection of prebuilt machine learning models you can use for common machine learning tasks such as object detection in images. The models are offered as an application programming interface (API), so you can easily integrate a model with your application. Some models can be customized with your own training data, saving time and resources to train a new model from scratch. Learn more about [Azure AI Services](https://learn.microsoft.com/en-us/azure/cognitive-services/what-are-cognitive-services).



Difference between services

- Use Azure AI Services whenever one of the customizable prebuilt models suits your requirements, to **save time and effort**.
- Use Azure Synapse Analytics or Azure Databricks if you want to **keep all data-related** (data engineering and data science) **projects within the same service**.
- Use Azure Synapse Analytics or Azure Databricks if you need **distributed compute** for working with large datasets (datasets are large when you experience capacity constraints with standard compute). You'll need to work with [PySpark](https://spark.apache.org/docs/latest/api/python) to use the distributed compute.
- Use Azure Machine Learning or Azure Databricks when you want **full control** over model training and management.
- Use Azure Machine Learning when **Python** is your preferred programming language.
- Use Azure Machine Learning when you want an **intuitive user interface** to manage your machine learning lifecycle.



## Decide between compute options

CPU vs GPU vs Spark

If you need **real-time predictions**, you need compute that is always available and able to return the results (almost) immediately. **Container** technologies like *Azure Container Instance* (ACI) and *Azure Kubernetes Service* (AKS) are ideal for such scenarios as they provide a lightweight infrastructure for your deployed model.

Alternatively, if you need **batch predictions**, you need compute that can handle a large workload. Ideally, you'd use a **compute cluster** that can score the data in *parallel* batches by using multiple nodes.





# Design a model deployment solution

To integrate the model, you need to deploy a model to an **endpoint**. Two options

- Get **real-time** predictions
  - If you want the model to score any new data as it comes in, you need predictions in real-time.
  - Real-time predictions are often needed when a model is used by an application such as a mobile app or a website.
- Get **batch** predictions
  - If you want the model to score new data in batches, and save the results as a file or in a database, you need batch predictions.
  - Imagine you're visualizing all historical sales data in a report

Whether you want real-time or batch predictions *doesn't necessarily depend on how often new data is collected*. Instead, it depends on how often and how quickly you need the predictions to be generated.



If you need **real-time predictions**, you need compute that is always available and able to return the results (almost) immediately. **Container** technologies like *Azure Container Instance* (ACI) and *Azure Kubernetes Service* (AKS) are ideal for such scenarios as they provide a lightweight infrastructure for your deployed model.

Alternatively, if you need **batch predictions**, you need compute that can handle a large workload. Ideally, you'd use a **compute cluster** that can score the data in *parallel* batches by using multiple nodes.



# Design a ML operations solution

Implementing MLOps helps you to make your machine learning workloads robust and reproducible.

- Convert the model training to a **robust** and **reproducible** pipeline.
- Test the code and the model in a **development** environment.
- Deploy the model in a **production** environment.
- **Automate** the end-to-end process.



## +Set up environments for development and production

**Environment** refers to a collection of resources. These resources are used to deploy an application, or with machine learning projects, to deploy a model.

A typical approach is to:

- Experiment with model training in the *development* environment.
- Move the best model to the *staging* or *pre-prod* environment to deploy and test the model.
- Finally release the model to the *production* environment to deploy the model so that end-users can consume it.



## +Design an MLOps architecture

- Store all data in an Azure Blob storage, managed by the data engineer.
- The infrastructure team creates all necessary Azure resources, like the Azure Machine Learning workspace.
- Data scientists focus on what they do best: developing and training the model (inner loop).
- Machine learning engineers deploy the trained models (outer loop).



1. **Setup**: Create all necessary Azure resources for the solution.
2. **Model development (inner loop)**: Explore and process the data to train and evaluate the model.
3. **Continuous integration**: Package and register the model.
4. **Model deployment (outer loop)**: Deploy the model.
5. **Continuous deployment**: Test the model and promote to production environment.
6. **Monitoring**: Monitor model and endpoint performance.



## +Design for retraining

Ideally, you should train models with **scripts** instead of notebooks. Scripts are better suited for automation. You can add **parameters** to a script



# Configure Azure ML workspace

To create an Azure Machine Learning service, you'll have to:

1. Get access to **Azure**, for example through the Azure portal.

2. Sign in to get access to an **Azure subscription**.

3. Create a **resource group** within your subscription.

4. Create an **Azure Machine Learning service** to create a workspace.

   - Use the user interface in the **Azure portal** to create an Azure Machine Learning service.
   - Create an **Azure Resource Manager** (**ARM**) template. [Learn how to use an ARM template to create a workspace](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-workspace-template?tabs=azcli%3Fazure-portal%3Dtrue).
   - Use the **Azure Command Line Interface** (**CLI**) with the Azure Machine Learning CLI extension. [Learn how to create the workspace with the CLI v2](https://learn.microsoft.com/en-us/training/modules/create-azure-machine-learning-resources-cli-v2/).
   - Use the **Azure Machine Learning Python SDK**.

   When a workspace is provisioned, Azure will automatically create other Azure resources within the same resource group to support the workspace:

   - **Azure Storage Account**: To store files and notebooks used in the workspace, and to store metadata of jobs and models.

   - **Azure Key Vault**: To securely manage secrets such as authentication keys and credentials used by the workspace.

   - **Application Insights**: To monitor predictive services in the workspace.

   - **Azure Container Registry**: Created when needed to store images for Azure Machine Learning environments.

![Diagram of hierarchy of Azure resources needed for the Azure Machine Learning workspace.](https://learn.microsoft.com/en-us/training/wwl-azure/explore-azure-machine-learning-workspace-resources-assets/media/overview-azure-resources.png)

## +**Role-based access control** (**RBAC**)

which you can configure in the **Access control** tab

- **Owner**: Gets full access to all resources, and can grant access to others using access control.
- **Contributor**: Gets full access to all resources, but can't grant access to others.
- **Reader**: Can only view the resource, but isn't allowed to make any changes.

Additionally, Azure Machine Learning has specific built-in roles you can use:

- **AzureML Data Scientist**: Can perform all actions within the workspace, except for creating or deleting compute resources, or editing the workspace settings.
- **AzureML Compute Operator**: Is allowed to create, change, and manage access the compute resources within a workspace.



# Identify Azure Machine Learning resources

The resources in Azure Machine Learning include:

- *The workspace*
  - The **workspace** is the top-level resource for Azure Machine Learning. Data scientists need access to the workspace to train and track models, and to deploy the models to endpoints.
- *Compute resources*
- *Datastores*



## +Create and manage compute resources

- **Compute instances**: Similar to a virtual machine in the cloud, managed by the workspace. Ideal to use as a development environment to run (Jupyter) notebooks.
- **Compute clusters**: On-demand clusters of CPU or GPU compute nodes in the cloud, managed by the workspace. Ideal to use for production workloads as they automatically scale to your needs.
- **Kubernetes clusters**: Allows you to create or attach an Azure Kubernetes Service (AKS) cluster. Ideal to deploy trained machine learning models in production scenarios.
- **Attached computes**: Allows you to attach other Azure compute resources to the workspace, like Azure Databricks or Synapse Spark pools.
- **Serverless compute**: A fully managed, on-demand compute you can use for training jobs.



## +Create and manage datastores

- `workspaceartifactstore`: Connects to the `azureml` container of the Azure Storage account created with the workspace. Used to store compute and experiment logs when running jobs.
- `workspaceworkingdirectory`: Connects to the file share of the Azure Storage account created with the workspace used by the **Notebooks** section of the studio. Whenever you upload files or folders to access from a compute instance, it's uploaded to this file share.
- `workspaceblobstore`: Connects to the Blob Storage of the Azure Storage account created with the workspace. Specifically the `azureml-blobstore-...` container. Set as the default datastore, which means that whenever you create a data asset and upload data, it's stored in this container.
- `workspacefilestore`: Connects to the file share of the Azure Storage account created with the workspace. Specifically the `azureml-filestore-...` file share.



## + Identify Azure Machine Learning assets

Assets are created and used at various stages of a project and include:

- Models
  - pickle file (`.pkl` extension) for storage
  - when create a **model** in the workspace, specify the *name* and *version*
- Environments
  - Environments specify software packages, environment variables, and software settings to run scripts
- Data
  - You can use data assets to easily access data every time, without having to provide authentication 
  - Path, name, and version
- Components
  - Reuse code basically
  - you have to specify the *name*, *version*, code, and *environment*
  - You can use components when creating **pipelines**. A component therefore often represents a step in a pipeline



## +Train models in the **workspace**

To train models with the Azure Machine Learning workspace, you have several options:

- Use **Automated Machine Learning**.
  - Automated Machine Learning iterates through algorithms paired with feature selections to find the best performing model for your data
- Run a Jupyter notebook.
  - The **Notebooks** page in the studio allows you to edit and run Jupyter notebooks.
- Run a script as a job..
  - You can run a script as a **job** in Azure Machine Learning. When you submit a job to the workspace, all inputs and outputs will be stored in the workspace.

There are different types of jobs depending on how you want to execute a workload:

1. **Command**: Execute a single script.
2. **Sweep**: Perform hyperparameter tuning when executing a single script.
3. **Pipeline**: Run a pipeline consisting of multiple scripts or components.



# Explore developer tools for workspace interaction

After the Python SDK is installed, you'll need to connect to the workspace

To authenticate, you need the values to three necessary parameters:

- `subscription_id`: Your subscription ID.
- `resource_group`: The name of your resource group.
- `workspace_name`: The name of your workspace.

Next, you can define the authentication by using the following code:

![image-20240629135020498](C:\Users\bread\AppData\Roaming\Typora\typora-user-images\image-20240629135020498.png)

After defining the authentication, you need to call `MLClient` for the environment to connect to the workspace. You'll call `MLClient` anytime you want to create or update an asset or resource in the workspace.

```
returned_job = ml_client.create_or_update(job)
```



## +Explore the CLI (command-line interface)

The Azure CLI is commonly used by administrators and engineers to automate tasks in Azure.

The Azure CLI allows you to:

- Automate the creation and configuration of assets and resources to make it **repeatable**.
- Ensure **consistency** for assets and resources that must be replicated in multiple environments (for example, development, test, and production).
- Incorporate machine learning asset configuration into developer operations (**DevOps**) **workflows**, such as **continuous integration** and **continuous deployment** (**CI/CD**) pipelines.



## +Install the Azure Machine Learning extension

Use Azure Machine Learning extension to manage Azure Machine Learning resources using the Azure CLI.

You can install the Azure Machine Learning extension `ml` with the following command:

```
az extension add -n ml -y

az ml -h
```



# Make data available in Azure Machine Learning

## +Understand URIs

- `http(s)`: Use for data stores publicly or privately in an Azure Blob Storage or publicly available http(s) location.
- `abfs(s)`: Use for data stores in an Azure Data Lake Storage Gen 2.
- `azureml`: Use for data stored in a datastore.



## +Create a datastore

In Azure Machine Learning, **datastores** are abstractions for cloud data sources. They encapsulate the information needed to connect to data sources, and securely store this connection information so that you don’t have to code it in your scripts. (Datastores allow you to **easily connect** to storage services)

Azure Machine Learning supports the creation of datastores for multiple kinds of Azure data source, including:

- Azure Blob Storage
- Azure File Share
- Azure Data Lake (Gen 2)

Every workspace has **four** built-in datastores (two connecting to Azure Storage blob containers, and two connecting to Azure Storage file shares), which are used as system storages by Azure Machine Learning.

In most machine learning projects, you need to work with data sources of your own.

You can create a datastore through the **graphical user interface**, the Azure command-line interface (**CLI**), or the Python software development kit (**SDK**).

```
blob_datastore = AzureBlobDatastore(
    			name = "blob_example",
    			description = "Datastore pointing to a blob container",
    			account_name = "mytestblobstore",
    			container_name = "data-container",
    			credentials = AccountKeyConfiguration(
        			account_key="XXXxxxXXXxXXXXxxXXX"
    			),
)
ml_client.create_or_update(blob_datastore)
```



## +Create a data asset

To simplify getting access to the data you want to work with, you can use **data assets**.

*data assets are references to where the data is stored, how to get access, and any other relevant metadata*

- You can **share and reuse data** with other members of the team such that they don't need to remember file locations.
- You can **seamlessly access data** during model training (on any supported compute type) without worrying about connection strings or data paths.
- You can **version** the metadata of the data asset.