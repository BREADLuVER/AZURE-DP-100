# Design a machine learning solution

## +Design a data ingestion strategy for ML projects

Extract data from a source and make it available to the Azure service

Extract, Transform, and Load **(*ETL*)**

Before being able to design the ETL or ELT process, you’ll need to identify your ***data source*** and ***data format**.*

Identify data source (storage)

- Customer Relationship Management (**CRM**) system
- transactional database like an **SQL database**
- generated by an Internet of Things (**IoT**) device.

Identify data format (storage)

- **Tabular** or **structured** data: All data has the same fields or properties, which are defined in a schema (Excel or CSV file)

- **Semi-structured** data: Instead, each data point is represented by a collection of *key-value pairs*. (loT generate a JSON object)
- **Unstructured** data: Files that don't adhere to any rules when it comes to structure. For example, documents, images, audio, and video files



## +Choose how to serve data to ML workflows

It's best practice to separate compute from storage

When you use **Azure Machine Learning**, **Azure Databricks**, or **Azure Synapse Analytics** for *model training*, there are three common options for storing data, which are easily connected to all three services:

- **Azure Blob Storage**: Cheapest option for storing data as *unstructured* data. Ideal for storing files like images, text, and JSON. Often also used to store data as CSV files, as data scientists prefer working with CSV files.
- **Azure Data Lake Storage (Gen 2)**: A more advanced version of the Azure Blob Storage. Also stores files like CSV files and images as *unstructured* data. A data lake also implements a hierarchical namespace, which means it’s easier to give someone access to a specific file or folder. Storage capacity is virtually limitless so ideal for storing large data.
- **Azure SQL Database**: Stores data as *structured* data. Data is read as a table and schema is defined when a table in the database is created. Ideal for data that doesn’t change over time.



## +Design a data ingestion pipeline

### Azure Synapse Analytics (**Azure Synapse Pipelines**)

- copy data from one source to a data store using connectors
- use **mapping data flow** or use a language like SQL, Python, or R for data transformation
- choose between different types of compute that can handle large data transformations at scale: server-less SQL pools, dedicated SQL pools, or Spark pools.

### Azure Data bricks

Azure Data-bricks allows you to define your pipelines in a notebook or use code-first tools like SQL, Python, or R to create your pipelines, which you can schedule to run. uses Spark clusters

### Azure Machine Learning

Create a pipeline with the Designer, or by creating a collection of scripts. Can also extract, transform, and store the data in preparation for training. Uses clusters 

Whenever you want to perform *all tasks within the same tool*, creating and scheduling an Azure Machine Learning pipeline to run with the on-demand compute cluster may best suit your needs.



### Design a data ingestion solution

data ingestion steps:

1. Extract raw data from its source (like a CRM system or IoT device).
2. Copy and transform the data with Azure Synapse Analytics.
3. Store the prepared data in an Azure Blob Storage.
4. Train the model with Azure Machine Learning.



![Diagram that shows data extracted, transformed with Azure Synapse Analytics, stored in a Storage Account, and served to Azure Machine Learning.](https://learn.microsoft.com/en-us/training/wwl-data-ai/design-data-ingestion-strategy-for-machine-learning-projects/media/04-01-pipeline.png)

# Design machine learning

1. **Classification**: Predict a categorical value.
2. **Regression**: Predict a numerical value.
3. **Time-series forecasting**: Predict future numerical values based on time-series data.
4. **Computer vision**: Classify images or detect objects in images.
5. **Natural language processing** (**NLP**): Extract insights from text.





If you need **real-time predictions**, you need compute that is always available and able to return the results (almost) immediately. **Container** technologies like *Azure Container Instance* (ACI) and *Azure Kubernetes Service* (AKS) are ideal for such scenarios as they provide a lightweight infrastructure for your deployed model.

Alternatively, if you need **batch predictions**, you need compute that can handle a large workload. Ideally, you'd use a **compute cluster** that can score the data in *parallel* batches by using multiple nodes.